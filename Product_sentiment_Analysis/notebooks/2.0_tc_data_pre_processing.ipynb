{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import chisquare\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import numpy as np\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\Git Projects\\twitter_product_sentiment\\data\\raw\n",
      "G:\\Git Projects\\twitter_product_sentiment\\data\\processed\n",
      "G:\\Git Projects\\twitter_product_sentiment\\data\\interim\n"
     ]
    }
   ],
   "source": [
    "# create generic path using so that the code can run in both windows and linux systems\n",
    "raw_data_path = os.path.abspath(os.path.join(os.getcwd(),os.path.pardir,'data','raw'))\n",
    "processed_data_path = os.path.abspath(os.path.join(os.getcwd(),os.path.pardir,'data','processed'))\n",
    "interim_data_path = os.path.abspath(os.path.join(os.getcwd(),os.path.pardir,'data','interim'))\n",
    "print(raw_data_path)\n",
    "print(processed_data_path)\n",
    "print(interim_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_train = pd.read_csv(os.path.abspath(os.path.join(interim_data_path,'train.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_test = pd.read_csv(os.path.abspath(os.path.join(interim_data_path,'test.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'tweet_id', 'tweet', 'sentiment', 'charcount',\n",
       "       'countwords', '@counts', '#counts', 'Capscounts',\n",
       "       'count_excl_quest_marks', 'count_urls ', 'count_special_chars',\n",
       "       'Company'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_req_columns(df):\n",
    "    return df[['tweet', 'sentiment', 'charcount','countwords', '@counts', '#counts', 'Capscounts','count_excl_quest_marks', 'count_urls ', 'count_special_chars','Company']]\n",
    "\n",
    "def return_req_columns_test(df):\n",
    "    return df[['tweet', 'charcount','countwords', '@counts', '#counts', 'Capscounts','count_excl_quest_marks', 'count_urls ', 'count_special_chars','Company']]\n",
    "\n",
    "def remove_tags(text, prefix):\n",
    "    '''remove # tags'''\n",
    "    text = re.sub(r'#\\w+',\"\",text) # remove the # tags\n",
    "    prefix+='_rt'\n",
    "    return [text,prefix]\n",
    "\n",
    "def remove_mentions(text,prefix):\n",
    "    '''remove @ mention'''\n",
    "    text = re.sub(r'@\\w+',\"\",text) # remove the # tags\n",
    "    prefix+='_rm'\n",
    "    return text,prefix\n",
    "\n",
    "def remove_english_stopwords(text,prefix):\n",
    "    '''Removes english stopwords'''\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    text = ' '.join([i for i in text.split() if i not in stop_words])\n",
    "    prefix+='_res'\n",
    "    return text,prefix\n",
    "\n",
    "def remove_stopwords(text,prefix):\n",
    "    '''keeps requiresd words from stopwords'''\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    required_words = ['was', 'did', 'but', 'against', 'not', 'no' , 'nor', 'over', 'under', 'again', 'few', 'more', 'most', 'too', 'very', 'couldn',\n",
    "    'couldn\\'t', 'don\\'t','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",\n",
    "    'haven',\"haven't\",'isn',\"isn't\",'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",\n",
    "    'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"]\n",
    "    stop_words = [i for i in stop_words if i not in required_words]\n",
    "    text = ' '.join([i for i in text.split() if i not in stop_words])\n",
    "    prefix+='_rs'\n",
    "    return text,prefix\n",
    "\n",
    "def remove_lest_than_3(text,prefix):\n",
    "    '''Removes words with length less than 2'''\n",
    "    text = ' '.join([i for i in text.split() if len(i)>2])\n",
    "    prefix+='_rlt3'\n",
    "    return text,prefix\n",
    "\n",
    "def apply_lemmatization(text, lemmatizer):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    data_post_lemm = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "    return data_post_lemm\n",
    "\n",
    "\n",
    "def apply_stemmer(text, stemm):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    data_post_stemm = ' '.join([stemm.stem(word) for word in words])\n",
    "    return data_post_stemm\n",
    "\n",
    "def stemming_all(df,prefix):    \n",
    "    df_tweet_clean_wordnet_stemmer=df\n",
    "    df_tweet_clean_porter_stemmer=df\n",
    "    df_tweet_clean_snowball_stemmer=df\n",
    "    df_tweet_clean_lancaster_stemmer=df\n",
    "    df_tweet_clean_wordnet_stemmer['tweet'] = df.tweet.apply(lambda x : apply_lemmatization(x,wordnet_lemmatizer))\n",
    "    df_tweet_clean_wordnet_stemmer.to_csv(os.path.abspath(os.path.join(processed_data_path,prefix+'_df_tweet_clean_wordnet_stemmer.csv'))) \n",
    "    df_tweet_clean_porter_stemmer['tweet'] = df.tweet.apply(lambda x : apply_stemmer(x,porter_stemmer))\n",
    "    df_tweet_clean_porter_stemmer.to_csv(os.path.abspath(os.path.join(processed_data_path,prefix+'_df_tweet_clean_porter_stemmer.csv')))\n",
    "    df_tweet_clean_snowball_stemmer['tweet'] = df.tweet.apply(lambda x : apply_stemmer(x,snowball_stemmer))\n",
    "    df_tweet_clean_snowball_stemmer.to_csv(os.path.abspath(os.path.join(processed_data_path,prefix+'_df_tweet_clean_snowball_stemmer.csv')))\n",
    "    df_tweet_clean_lancaster_stemmer['tweet'] = df.tweet.apply(lambda x : apply_stemmer(x,lancaster_stemmer))\n",
    "    df_tweet_clean_lancaster_stemmer.to_csv(os.path.abspath(os.path.join(processed_data_path,prefix+'_df_tweet_clean_lancaster_stemmer.csv')))\n",
    "    return df_tweet_clean_wordnet_stemmer,df_tweet_clean_porter_stemmer,df_tweet_clean_snowball_stemmer,df_tweet_clean_lancaster_stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dispatcher = { 'remove_tags' : remove_tags, 'remove_mentions' : remove_mentions, 'remove_english_stopwords': remove_english_stopwords, 'remove_stopwords': remove_stopwords, 'remove_lest_than_3': remove_lest_than_3, 'apply_lemmatization': apply_lemmatization, 'apply_stemmer': apply_stemmer}\n",
    "\n",
    "\n",
    "def call_func(text, prefix, func):\n",
    "    try:\n",
    "        return dispatcher[func](text, prefix)\n",
    "    except:\n",
    "        return \"Invalid function\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt(df,prefix):\n",
    "    functions = ['remove_tags','remove_mentions','remove_english_stopwords','remove_stopwords','remove_lest_than_3','stemming_all']\n",
    "    for func_name in functions:\n",
    "        print('\\ncalling function',func_name)\n",
    "        print('\\nprefix before calling: ',prefix)\n",
    "        if func_name == 'stemming_all':\n",
    "            stemming_all(df,prefix)\n",
    "        else:\n",
    "            df.tweet = df.tweet.apply(lambda x: call_func(x, prefix,func_name)[0])\n",
    "            txt,prefix = call_func(' abcd ',prefix,func_name)\n",
    "        print('\\nprefix after calling: ',prefix)\n",
    "        df.to_csv(os.path.abspath(os.path.join(processed_data_path,prefix+'df.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "calling function remove_tags\n",
      "\n",
      "prefix before calling:  train\n",
      "\n",
      "prefix after calling:  train_rt\n",
      "\n",
      "calling function remove_mentions\n",
      "\n",
      "prefix before calling:  train_rt\n",
      "\n",
      "prefix after calling:  train_rt_rm\n",
      "\n",
      "calling function remove_english_stopwords\n",
      "\n",
      "prefix before calling:  train_rt_rm\n",
      "\n",
      "prefix after calling:  train_rt_rm_res\n",
      "\n",
      "calling function remove_stopwords\n",
      "\n",
      "prefix before calling:  train_rt_rm_res\n",
      "\n",
      "prefix after calling:  train_rt_rm_res_rs\n",
      "\n",
      "calling function remove_lest_than_3\n",
      "\n",
      "prefix before calling:  train_rt_rm_res_rs\n",
      "\n",
      "prefix after calling:  train_rt_rm_res_rs_rlt3\n",
      "\n",
      "calling function stemming_all\n",
      "\n",
      "prefix before calling:  train_rt_rm_res_rs_rlt3\n",
      "\n",
      "prefix after calling:  train_rt_rm_res_rs_rlt3\n"
     ]
    }
   ],
   "source": [
    "df_train = return_req_columns(df_train)\n",
    "process_txt(df_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "calling function remove_tags\n",
      "\n",
      "prefix before calling:  test\n",
      "\n",
      "prefix after calling:  test_rt\n",
      "\n",
      "calling function remove_mentions\n",
      "\n",
      "prefix before calling:  test_rt\n",
      "\n",
      "prefix after calling:  test_rt_rm\n",
      "\n",
      "calling function remove_english_stopwords\n",
      "\n",
      "prefix before calling:  test_rt_rm\n",
      "\n",
      "prefix after calling:  test_rt_rm_res\n",
      "\n",
      "calling function remove_stopwords\n",
      "\n",
      "prefix before calling:  test_rt_rm_res\n",
      "\n",
      "prefix after calling:  test_rt_rm_res_rs\n",
      "\n",
      "calling function remove_lest_than_3\n",
      "\n",
      "prefix before calling:  test_rt_rm_res_rs\n",
      "\n",
      "prefix after calling:  test_rt_rm_res_rs_rlt3\n",
      "\n",
      "calling function stemming_all\n",
      "\n",
      "prefix before calling:  test_rt_rm_res_rs_rlt3\n",
      "\n",
      "prefix after calling:  test_rt_rm_res_rs_rlt3\n"
     ]
    }
   ],
   "source": [
    "df_test = return_req_columns_test(df_test)\n",
    "process_txt(df_train,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
